{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DilkiIshara/Hate-Speech-Detection-System/blob/main/Hate%20Speech%20Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lbDqoI-tB43P",
        "outputId": "01dbdb03-d5f0-42c5-bef1-e36b8453bbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install scikit-learn\n",
        "!pip install torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL6GTch8CKF9",
        "outputId": "9e819f56-3c83-4587-e8af-b665867cb95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined translated dataset already exists at /content/drive/MyDrive/HSDData/Translated_Sinhala.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict as ddict\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# from transformers import (AdamW, XLMRobertaForSequenceClassification,\n",
        "#                           XLMRobertaTokenizer, get_linear_schedule_with_warmup)\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, MarianMTModel, MarianTokenizer, AutoTokenizer,AutoModelForSequenceClassification, pipeline, AutoModelForSeq2SeqLM\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "\n",
        "# Load tokenizer and model for translation (English to Sinhala)\n",
        "translation_model_name = \"thilina/mt5-sinhalese-english\"\n",
        "translation_tokenizer = AutoTokenizer.from_pretrained(translation_model_name)\n",
        "translation_model = AutoModelForSeq2SeqLM.from_pretrained(translation_model_name)\n",
        "\n",
        "# Paths for English dataset and output\n",
        "english_data_path = '/content/drive/MyDrive/HSDData/EnglishDataSet.csv'\n",
        "sinhala_data_path = '/content/drive/MyDrive/HSDData/SinhalaDataSet.tsv'\n",
        "translated_data_dir = '/content/drive/MyDrive/HSDData/Translated_Batches/'\n",
        "translated_sinhala_file = '/content/drive/MyDrive/HSDData/Translated_Sinhala.csv'\n",
        "\n",
        "os.makedirs(translated_data_dir, exist_ok=True)\n",
        "\n",
        "# Translate function with retries and delay\n",
        "def translate_to_sinhala(text):\n",
        "    retries = 2\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            inputs = translation_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "            translated_tokens = translation_model.generate(inputs, max_length=1000)\n",
        "            translated_text = translation_tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "            time.sleep(1)  # Add delay to avoid rate limiting\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed for text: {text} - {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(2)  # Wait before retrying\n",
        "            else:\n",
        "                print(f\"Translation failed after {retries} attempts for text: {text}\")\n",
        "                return text  # Return original text if all retries fail\n",
        "\n",
        "# Translate dataset in chunks\n",
        "def translate_in_chunks(data, chunk_size=1000):\n",
        "    for i in range(0, len(data), chunk_size):\n",
        "        chunk = data[i:i + chunk_size]\n",
        "        chunk_file = os.path.join(translated_data_dir, f'translated_ML_batch_{i // chunk_size + 1}.csv')\n",
        "\n",
        "        if os.path.exists(chunk_file):\n",
        "            print(f\"Batch {i // chunk_size + 1} already translated, skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Translating batch {i // chunk_size + 1}...\")\n",
        "        chunk['translated_text'] = chunk['text'].apply(translate_to_sinhala)\n",
        "        chunk[['translated_text', 'label']].to_csv(chunk_file, index=False)\n",
        "        print(f\"Batch {i // chunk_size + 1} saved to {chunk_file}\")\n",
        "\n",
        "# Check if the combined translated file exists\n",
        "if not os.path.exists(translated_sinhala_file):\n",
        "    # Load the English dataset\n",
        "    english_data = pd.read_csv(english_data_path)\n",
        "\n",
        "    # Translate in chunks\n",
        "    #translate_in_chunks(english_data, chunk_size=1000)\n",
        "\n",
        "    # Combine translated batches\n",
        "    translated_batches = [\n",
        "        pd.read_csv(os.path.join(translated_data_dir, f))\n",
        "        for f in sorted(os.listdir(translated_data_dir))\n",
        "        if f.startswith('translated_ML_batch_')\n",
        "    ]\n",
        "    english_data_translated = pd.concat(translated_batches, ignore_index=True)\n",
        "\n",
        "    # Save combined translated file\n",
        "    english_data_translated.to_csv(translated_sinhala_file, index=False)\n",
        "    print(f\"Combined translated dataset saved to {translated_sinhala_file}\")\n",
        "else:\n",
        "    print(f\"Combined translated dataset already exists at {translated_sinhala_file}\")\n",
        "    english_data_translated = pd.read_csv(translated_sinhala_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcDXGjPeOr6L"
      },
      "source": [
        "Load sinhala Data set and Combine those"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9VHOgdSCXQ2",
        "outputId": "ff29d2b3-82e4-4bd6-d00c-ece1aaa41d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset saved to /content/drive/MyDrive/HSDData/Combined_Sinhala_Data.csv\n"
          ]
        }
      ],
      "source": [
        "combined_data_file = '/content/drive/MyDrive/HSDData/Combined_Sinhala_Data.csv'\n",
        "sinhala_data = pd.read_csv(sinhala_data_path, sep='\\t')\n",
        "# Convert Sinhala labels from NOT/OFF to numeric\n",
        "sinhala_data['label'] = sinhala_data['label'].map({'NOT': 0, 'OFF': 1})\n",
        "\n",
        "# Select only the necessary columns from SinhalaDataSet\n",
        "sinhala_data = sinhala_data[['text', 'label']]\n",
        "\n",
        "# Rename column in Translated_Sinhala\n",
        "english_data_translated = english_data_translated.rename(columns={\"translated_text\": \"text\"})\n",
        "\n",
        "# Combine datasets\n",
        "combined_data = pd.concat([sinhala_data, english_data_translated], ignore_index=True)\n",
        "\n",
        "# Save combined dataset\n",
        "combined_data.to_csv(combined_data_file, index=False)\n",
        "print(f\"Combined dataset saved to {combined_data_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbA939hQU7vP"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uKCAwcDU9r7",
        "outputId": "ca4a7b6f-e5b5-433c-f92f-782fb6d0a453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 166 Sinhala stopwords from file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sinhala_stopwords_path = '/content/drive/MyDrive/HSDData/Sinhala_Stop_Words.txt'\n",
        "\n",
        "# Load Sinhala stopwords from file\n",
        "with open(sinhala_stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    sinhala_stopwords = set(f.read().splitlines())\n",
        "print(f\"Loaded {len(sinhala_stopwords)} Sinhala stopwords from file.\")\n",
        "\n",
        "# Preprocess text based on language\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english') and word not in sinhala_stopwords])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to text\n",
        "combined_data['cleaned_text'] = combined_data['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW4JXReOc-CZ",
        "outputId": "0b93b4e0-b030-4146-b3ae-d1880be2d322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Split into features and labels\n",
        "X = combined_data['cleaned_text']\n",
        "y = combined_data['label']\n",
        "\n",
        "# Ensure labels are numeric\n",
        "y = y.astype(int)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and prepare data for XLM-R\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# Tokenize training and test sets\n",
        "X_train_tokenized = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "X_test_tokenized = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Load pre-trained XLM-R model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
        "\n",
        "# Prepare DataLoader for training\n",
        "class HateSpeechDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are tensors with correct type\n",
        "        return item\n",
        "\n",
        "train_dataset = HateSpeechDataset(X_train_tokenized, list(y_train))\n",
        "test_dataset = HateSpeechDataset(X_test_tokenized, list(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvvhIQ9mdpnu"
      },
      "source": [
        "Training Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "wuqIzlILdaN6",
        "outputId": "993ccca2-e088-4e23-c725-3be1dc75227d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4062a829e016>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 3  # 3 epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    model.train()\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    for epoch in range(3):  # Train for 3 epochs\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = loss_fn(outputs.logits, batch['labels'])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "        model.save_pretrained(f\"/content/drive/MyDrive/HateSpeech_XLMR_Model_Epoch_{epoch}\")\n",
        "        tokenizer.save_pretrained(f\"/content/drive/MyDrive/HateSpeech_XLMR_Model_Epoch_{epoch}\")\n",
        "\n",
        "train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            y_preds.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
        "            y_trues.extend(batch['labels'].tolist())\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_trues, y_preds))\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_trues, y_preds))\n",
        "\n",
        "# Run training and evaluation\n",
        "train_model()\n",
        "evaluate_model()\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/HateSpeech_XLMR_Model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/HateSpeech_XLMR_Model\")"
      ],
      "metadata": {
        "id": "VA3AiAUqxvp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1KUZibvNHgp1hR2Fz4BB6eHHv1LgJEb6x",
      "authorship_tag": "ABX9TyOnEJcnRo/Cuq3sbMitY8A/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}